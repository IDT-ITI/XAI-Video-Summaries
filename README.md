# An Integrated Framework for Multi-Granular Explanation of Video Summarization

## PyTorch implementation of the software used in:
- **"An Integrated Framework for Multi-Granular Explanation of Video Summarization"**, Proc. TBA
- Written by Konstantinos Tsigos, Evlampios Apostolidis, Spyridon Baxevanakis, Symeon Papadopoulos and Vasileios Mezaris.
- This software can be used to produce explanations for the outcome of a video summarization model. Our framework integrates methods for generating explanations at the fragment level (indicating which video fragments influenced the most the decisions of the summarizer), and the more fine-grained object level (highlighting which visual objects were the most influential for the summarizer on a specific video fragment). For the fragment level, we employ the model-specific Attention-based approach proposed in [Apostolidis et al. (2022)](https://ieeexplore.ieee.org/document/10019643), and introduce a new model-agnostic method, that does not require any knowledge about the summarization model. The fragments of the afformentioned explanations, alongside the fragments selected by the summarizer to be included in the summary, are then divided into distinguisable and coherent visual concepts using a state-of-the-art video panoptic segmentation framework and combined with an adaptation of a perturbation-based approach to generate the object level explanations.

## Main dependencies
The code was developed, checked and verified on an `Ubuntu 20.04.6` PC with an `NVIDIA RTX 4090` GPU and an `i5-12600K` CPU. All dependencies of this project can be found inside the [requirements.txt](requirements.txt) file, which can be used to set up the necessary virtual enviroment.

Since the code uses the TransNetV2 and video K-Net frameworks to temporally segment the videos and get the video panoptic segmentation of the fragments respectively, two additional projects need to be made containing the [TransNetV2](https://github.com/soCzech/TransNetV2) and [video K-Net](https://github.com/lxtGH/Video-K-Net) GitHub repositories, alongside the necessary virtual environments for them to work.

In the case of video K-Net:
- The trained model used is the `video_k_net_swinb_vip_seg.pth`, which can be downloaded from the VIP-Seg folder of the provided links found at the README [Pretrained CKPTs and Trained Models](https://github.com/lxtGH/Video-K-Net?tab=readme-ov-file#pretrained-ckpts-and-trained-models) section. It is then placed inside the root directory of the video K-Net project.
- The `test_step.py` python file located at the `/tools` directory, needs to be replaced with the modified [test_step.py](/k-Net/test_step.py) file, located into the [k-Net](/k-Net) folder of this project.
- The `data` folder needs to be created into the root directory of the video k-Net project with the following structure:

```Text
/data
    /VIPSeg
        /images
            /fragment
        /panomasks
            /fragment
        val.txt
```
where `val.txt` is a txt file containing the word fragment

## Data
<div align="justify">

Dataset providers' webpages:
<a href="https://github.com/yalesong/tvsum" target="_blank"><img align="center" src="https://img.shields.io/badge/Dataset-TVSum-green"/></a> <a href="https://gyglim.github.io/me/vsum/index.html#benchmark" target="_blank"><img align="center" src="https://img.shields.io/badge/Dataset-SumMe-blue"/></a>

In order for the code to work, the original videos of the datasets have to be downloaded and placed into the [SumMe](/data/SumMe) and [TVSum](/data/TVSum) folders located at the [data](data) directory.

The optical sub-shots generated by the proposed method from [Apostolidis et al. (2018)](https://link.springer.com/chapter/10.1007/978-3-319-73603-7_3), for videos with an insufficient amount of shots, are provided and can be located at the afformentioted folders.

To get the deep features for all the videos of the SumMe and TVSum datasets and save them into an h5 file, run the [feature_extraction.py](explanation/features/feature_extraction.py) script. Otherwise, the necessary h5 file will be produced and placed into the correspoding folder of each individual video.

The produced h5 files have the following structure:
```Text
/key
    /features                 2D-array with shape (n_steps, feature-dimension)
    /n_frames                 number of frames in original video
```

</div>

## Running an experiment
<div align="justify">

To run an experiment using one of the aforementioned datasets, execute the following command:

```
python explanation/explain.py --summarization_method 'method_name' --dataset 'dataset_name' --replacement_method 'replacement_method_name' --replaced_fragments 'set_of_repl_fragments' --visual_mask 'visual_mask_name'
```
where, `method_name` refers to the name of the used video summarization method, `dataset_name` refers to the name of the used dataset, `replacement_method_name` refers to the applied replacement function in fragments of the input data, `set_of_repl_fragments` refers to the amount of replaced fragments of the input data, and `visual_mask_name` refers to the type of the used mask when replacing fragments of the input data.

After executing the above command you get the overall results for each different data split, as well as the overall results that are computed by averaging the obtained scores across data splits. Please note that, the results when fragments' replacement is based on "Randomization" might be slightly different from the reported ones, as we did not use a fixed seed value in our experiments.

## Running parameters and evaluation results
<div align="justify">

Setup for the experimental evaluation:
 - In [`main.py`](main.py), specify the path to the pretrained models of the used method for video summarization. 
 - In [`data_loader.py`](data_loader.py), specify the paths to the h5 file of the used dataset, and the JSON file containing data about the used data splits.</div>
   
Arguments in [`configs.py`](configs.py): 
|Parameter name | Description | Default Value | Options
| :--- | :--- | :---: | :---:
`summarization method` | The used video summarization method. | 'CA-SUM' | 'CA-SUM', 'VASNet', 'SUM-GDA'
`dataset` | The used dataset. | 'SumMe' | 'SumMe', 'TVSum'
`replacement_method` | The applied replacement function. | 'slice-out' | 'slice-out', 'input-mask', 'random', 'attention-mask'
`replaced_fragments` | The amount of replaced fragments. | 'batch' | 'batch', 'single'
`visual_mask` | The used visual mask for replacement. | 'black-frame' | 'black-frame', 'white-frame'

## Citation
<div align="justify">
    
If you find our work, code or pretrained models, useful in your work, please cite the following publication:

K. Tsigos, E. Apostolidis, V. Mezaris, "<b>An Integrated Framework for Multi-Granular Explanation of Video Summarization</b>", Proc. TBA.
</div>

BibTeX:

```
@INPROCEEDINGS{TBA,
    author    = {Tsigos, Konstantinos, and Apostolidis, Evlampios and Mezaris, Vasileios},
    title     = {An Integrated Framework for Multi-Granular Explanation of Video Summarization},
    year      = {2024},
    publisher = {TBA},
    address   = {TBA},
    booktitle = {TBA},
    location  = {TBA},
    series    = {TBA}
}
```

## License
<div align="justify">
    
Copyright (c) 2024, Konstantinos Tsigos, Evlampios Apostolidis, Spyridon Baxevanakis, Symeon Papadopoulos, Vasileios Mezaris / CERTH-ITI. All rights reserved. This code is provided for academic, non-commercial use only. Redistribution and use in source and binary forms, with or without modification, are permitted for academic non-commercial use provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation provided with the distribution.

This software is provided by the authors "as is" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall the authors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.
</div>

## Acknowledgement
<div align="justify"> This work was supported by the EU Horizon Europe and Horizon 2020 programmes under grant agreements 101070190 AI4TRUST and 951911 AI4Media, respectively. </div>
