# An Integrated Framework for Multi-Granular Explanation of Video Summarization

## PyTorch implementation of the software used in:
- [**"An Integrated Framework for Multi-Granular Explanation of Video Summarization"**](https://arxiv.org/abs/2405.10082)
- Written by Konstantinos Tsigos, Evlampios Apostolidis, Spyridon Baxevanakis, Symeon Papadopoulos and Vasileios Mezaris.
- This software can be used to produce explanations for the outcome of a video summarization model. Our framework integrates methods for generating explanations at the fragment level (indicating which video fragments influenced the most the decisions of the summarizer), and the more fine-grained object level (highlighting which visual objects were the most influential for the summarizer on a specific video fragment). For the fragment level, we employ the model-specific Attention-based approach proposed in [Apostolidis et al. (2022)](https://ieeexplore.ieee.org/document/10019643), and introduce a new model-agnostic method, that does not require any knowledge about the summarization model. The fragments of the afformentioned explanations, alongside the fragments selected by the summarizer to be included in the summary, are then divided into distinguisable and coherent visual concepts using a state-of-the-art video panoptic segmentation framework and combined with an adaptation of a perturbation-based approach to generate the object level explanations.

## Main dependencies
The code was developed, checked and verified on an `Ubuntu 20.04.6` PC with an `NVIDIA RTX 4090` GPU and an `i5-12600K` CPU. All dependencies of this project can be found inside the [requirements.txt](requirements.txt) file, which can be used to set up the necessary virtual enviroment.

Since the code uses the TransNetV2 and video K-Net frameworks to temporally segment the videos and get the video panoptic segmentation of the fragments respectively, two additional projects need to be made containing the [TransNetV2](https://github.com/soCzech/TransNetV2) and [video K-Net](https://github.com/lxtGH/Video-K-Net) GitHub repositories, alongside the necessary virtual environments for them to work.

In the case of video K-Net:
- The trained model used is the `video_k_net_swinb_vip_seg.pth`, which can be downloaded from the VIP-Seg folder of the provided links found at the README [Pretrained CKPTs and Trained Models](https://github.com/lxtGH/Video-K-Net?tab=readme-ov-file#pretrained-ckpts-and-trained-models) section. It is then placed inside the root directory of the video K-Net project.
- The `test_step.py` python file located at the `/tools` directory, needs to be replaced with the modified [test_step.py](/k-Net/test_step.py) file, provided with in this project.
- The `data` folder needs to be created into the root directory of the video k-Net project with the following structure:

```Text
/data
    /VIPSeg
        /images
            /fragment
        /panomasks
            /fragment
        val.txt
```
where `val.txt` is a txt file containing the word fragment.

The paths of the TransNetV2 and video K-Net projects, along with their corresponding virtual environments can be set in the [video_segmentation.py](segmentation/video_segmentation.py#L7:L10) and [frame_segmentation.py](segmentation/frame_segmentation.py#L12:L15) files, accordingly. Do note that the paths for the projects are given relative to the parent directory of this project, while the paths of the virtual environments are given relative to the root directory of the corresponding project.


If you want to use the default paths:
- Set the name of the root directory of the projects to *TransNetV2* and *K-Net* and place them in the parent directory of this project.
- Set the name of the virtual environment of each project to *.venv* and place it inside the root directory of the corresponding project.

In the end you should end up with the following structure:
```Text
/Parent Directory
    /K-Net
        /.venv
        ...
    /TransNetV2
        /.venv
        ...
    /XAI-Video-Summaries
        ...
```

## Data
<div align="justify">

Dataset providers' webpages:
<a href="https://github.com/yalesong/tvsum" target="_blank"><img align="center" src="https://img.shields.io/badge/Dataset-TVSum-green"/></a> <a href="https://gyglim.github.io/me/vsum/index.html#benchmark" target="_blank"><img align="center" src="https://img.shields.io/badge/Dataset-SumMe-blue"/></a>

In order for the code to work, the original videos of the datasets have to be downloaded and placed into the [SumMe](/data/SumMe) and [TVSum](/data/TVSum) folders located at the [data](data) directory.

The optical sub-shots generated by the proposed method from [Apostolidis et al. (2018)](https://link.springer.com/chapter/10.1007/978-3-319-73603-7_3), for videos with an insufficient amount of shots, are provided and can be located at the afformentioted folders.

To get the deep features for all the videos of the SumMe and TVSum datasets and save them into an h5 file, run the [feature_extraction.py](features/feature_extraction.py) script. Otherwise, the necessary h5 file will be produced and placed into the correspoding folder of each individual video.

The produced h5 files have the following structure:
```Text
/key
    /features                 2D-array with shape (n_steps, feature-dimension)
    /n_frames                 number of frames in original video
```

</div>

## Running an experiment
<div align="justify">

To run an experiment using a video from one of the aforementioned datasets, execute the following command:

```
python explanation/explain.py --model MODEL_PATH --video VIDEO_PATH --fragments NUM_OF_FRAGMENTS (optional, default=3)
```
where, `MODEL_PATH` refers to the path of the checkpoint of the trained video summarizer, `VIDEO_PATH` refers to the path of the video, and `NUM_OF_FRAGMENTS` which refers to the number of video fragments to generate the explanations.

Two already trained video summarization models, one for each dataset, are provided inside the [models](/explanation/models) folder in the `explanation` directory and have the following characteristics:
Model| F1 score | Epoch | Split | Reg. Factor
| --- | --- | --- | --- | --- |
summe.pkl | 59.138 | 383 | 4 | 0.5
tvsum.pkl | 63.462 | 44 | 4 | 0.5

After executing the above command, a new folder is created (if it does not already exist), with the same name and in the same directory as the video we want to explain. There, the deep features (if the file containing the deep features for all the videos of the dataset, does not exist in the dataset folder) and the video shots (unless we already have the optical flow subshots) are extracted in h5 and txt format files, accordingly. Another folder is also created called explanation, containing two txt files that hold the fragment-level explanation (the top `NUM_OF_FRAGMENTS` from Attention and Positive LIME are the explanation) and the evaluation scores, one csv file with the indexes of the explanation fragments and three folders that have the object level explanations, using the top fragments obtained from the afformentioned fragment-level explanations, as well as the ones selected to be included in the summary by the summarizer. Each folder contains 4 explanation images for each fragment, highlighting the most positive, most negative and both positive and negative segments of the keyframe, as well as providing the mask of the latter. Finally, the evaluation scores of each object level explanation can be found inside a csv file, with each row corresponding to the metrics of the top fragments in descending order (first row: top 1 fragment, second row: top 2 fragment etc).

Alternatively, to run the experiment on all of the videos of both datasets, execute the [explain.sh](/explanation/explain.sh) bash script.

## Evaluation results
<div align="justify">

To get the evaluation results, averaged on all of the videos of each dataset, run the [final_scores.py](explanation/final_scores.py) script. The final scores are saved into the `final_scores` folder, created inside the [explanation](/explanation) path. To specify a specific dataset or videos for each dataset, set the [dataset](explanation/final_scores.py#L9:L10) and [videos](explanation/final_scores.py#L11:L12) variables appropriately.

## Citation
<div align="justify">
    
If you find our work, code or pretrained models, useful in your work, please cite the following publication:

K. Tsigos, E. Apostolidis, V. Mezaris, "<b>An Integrated Framework for Multi-Granular Explanation of Video Summarization</b>".
</div>

BibTeX:

```
@misc{tsigos2024integrated,
      title={An Integrated Framework for Multi-Granular Explanation of Video Summarization}, 
      author={Konstantinos Tsigos and Evlampios Apostolidis and Vasileios Mezaris},
      year={2024},
      eprint={2405.10082},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## License
<div align="justify">
    
Copyright (c) 2024, Konstantinos Tsigos, Evlampios Apostolidis, Spyridon Baxevanakis, Symeon Papadopoulos, Vasileios Mezaris / CERTH-ITI. All rights reserved. This code is provided for academic, non-commercial use only. Redistribution and use in source and binary forms, with or without modification, are permitted for academic non-commercial use provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation provided with the distribution.

This software is provided by the authors "as is" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall the authors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.
</div>

## Acknowledgement
<div align="justify"> This work was supported by the EU Horizon Europe and Horizon 2020 programmes under grant agreements 101070190 AI4TRUST and 951911 AI4Media, respectively. </div>
